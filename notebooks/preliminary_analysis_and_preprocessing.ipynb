{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gmaps\n",
    "import geohash\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "from geopy.distance import distance\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sns.set()\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "API_KEY = 'AIzaSyAVg4YdqUFQ5D5wHoq3GRW0xg2vgj5V9EE'\n",
    "gmaps.configure(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamp_hour(row):\n",
    "    timestamp_to_convert = row.timestamp.split(':')\n",
    "    hour = float(timestamp_to_convert[0])\n",
    "    minutes = float(timestamp_to_convert[1]) / 60\n",
    "    return row.day * 24 + hour + minutes\n",
    "    \n",
    "def get_timestamp_decimal(timestamp):\n",
    "    timestamp_to_convert = timestamp.split(':')\n",
    "    hour = float(timestamp_to_convert[0])\n",
    "    minutes = float(timestamp_to_convert[1])\n",
    "    return hour + minutes / 60\n",
    "\n",
    "def create_ts_decimal_lag(ts, lag):\n",
    "    if (ts  - lag * 0.25) < 0:\n",
    "        return ts - lag * 0.25 + 24.0\n",
    "    elif (ts  - lag * 0.25) > 23.75:\n",
    "        return ts - lag * 0.25 - 24.0\n",
    "    else:\n",
    "        return ts - lag  * 0.25\n",
    "    \n",
    "def replace_mistmatching_demand(row, lag):\n",
    "    if lag > 0:\n",
    "        if pd.isnull(row['d_t_plus_{}'.format(lag)]):\n",
    "            return np.nan\n",
    "        elif row['tdelta_plus_{}'.format(lag)] != 0.25 * lag:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['d_t_plus_{}'.format(lag)]\n",
    "    else:\n",
    "        if pd.isnull(row['d_t_minus_{}'.format(-lag)]):\n",
    "            return np.nan\n",
    "        elif row['tdelta_minus_{}'.format(-lag)] != 0.25 * lag:\n",
    "            return 0\n",
    "        else:\n",
    "            return row['d_t_minus_{}'.format(-lag)]\n",
    "\n",
    "def get_time_lags(df):\n",
    "    unique_geohash = df['geohash6'].unique()\n",
    "    temp = []\n",
    "    for idx, gh in enumerate(unique_geohash):\n",
    "        if idx % 50 == 0:\n",
    "            print('{}/{} processed'.format(idx +1, len(unique_geohash)))\n",
    "            \n",
    "        if idx+ 1 % len(unique_geohash) == 0:\n",
    "            print('{}/{} processed'.format(idx, len(unique_geohash)))\n",
    "\n",
    "        rel_gh = df.loc[df.geohash6 == gh].copy()\n",
    "        for t in range(1,6):\n",
    "            rel_gh['ts_plus_{}'.format(t)]  = rel_gh['timestamp_hour'].shift(-t)\n",
    "            rel_gh['tdelta_plus_{}'.format(t)] = rel_gh['ts_plus_{}'.format(t)] - rel_gh['timestamp_hour']\n",
    "            rel_gh['d_t_plus_{}'.format(t)] = rel_gh['demand'].shift(-t)\n",
    "            rel_gh['d_t_plus_{}'.format(t)] = rel_gh.apply(lambda x: replace_mistmatching_demand(x, t), axis=1)\n",
    "            \n",
    "            rel_gh['ts_minus_{}'.format(t)]  = rel_gh['timestamp_hour'].shift(t)\n",
    "            rel_gh['tdelta_minus_{}'.format(t)] = rel_gh['ts_minus_{}'.format(t)] - rel_gh['timestamp_hour']\n",
    "            rel_gh['d_t_minus_{}'.format(t)] = rel_gh['demand'].shift(t)\n",
    "            rel_gh['d_t_minus_{}'.format(t)] = rel_gh.apply(lambda x: replace_mistmatching_demand(x, -t), axis=1)\n",
    "            \n",
    "#         for t in range(6, 11):\n",
    "#             rel_gh['ts_minus_{}'.format(t)]  = rel_gh['timestamp_hour'].shift(t)\n",
    "#             rel_gh['tdelta_minus_{}'.format(t)] = rel_gh['ts_minus_{}'.format(t)] - rel_gh['timestamp_hour']\n",
    "#             rel_gh['d_t_minus_{}'.format(t)] = rel_gh['demand'].shift(t)\n",
    "#             rel_gh['d_t_minus_{}'.format(t)] = rel_gh.apply(lambda x: replace_mistmatching_demand(x, -t), axis=1)\n",
    "            \n",
    "        temp.append(rel_gh)\n",
    "    train_df = pd.concat(temp)\n",
    "    scaler = MinMaxScaler()\n",
    "    for lag in range(1, 6):\n",
    "        train_df['ts_d_minus_{}'.format(lag)] = train_df['timestamp_decimal'] \\\n",
    "            .apply(lambda x: create_ts_decimal_lag(x, lag))\n",
    "        train_df['ts_d_minus_{}_scaled'.format(lag)] = scaler \\\n",
    "            .fit_transform(train_df['ts_d_minus_{}'.format(lag)].values.reshape(-1, 1))\n",
    "    train_df = train_df[sorted(train_df.columns)]\n",
    "        \n",
    "    return train_df\n",
    "\n",
    "def get_exp_sample_weight(demand):\n",
    "    return np.exp(np.round(demand*10, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geohash6</th>\n",
       "      <th>day</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>demand</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>hour</th>\n",
       "      <th>timestamp_hour</th>\n",
       "      <th>lat_scaled</th>\n",
       "      <th>lon_scaled</th>\n",
       "      <th>d_t</th>\n",
       "      <th>sample_weight</th>\n",
       "      <th>timestamp_decimal</th>\n",
       "      <th>timestamp_decimal_scaled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>1</td>\n",
       "      <td>2:45</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-5.485</td>\n",
       "      <td>90.654</td>\n",
       "      <td>2</td>\n",
       "      <td>26.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.221</td>\n",
       "      <td>2.750</td>\n",
       "      <td>0.116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>1</td>\n",
       "      <td>3:0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-5.485</td>\n",
       "      <td>90.654</td>\n",
       "      <td>3</td>\n",
       "      <td>27.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.010</td>\n",
       "      <td>1.105</td>\n",
       "      <td>3.000</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>1</td>\n",
       "      <td>4:0</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-5.485</td>\n",
       "      <td>90.654</td>\n",
       "      <td>4</td>\n",
       "      <td>28.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.007</td>\n",
       "      <td>1.105</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>1</td>\n",
       "      <td>4:30</td>\n",
       "      <td>0.004</td>\n",
       "      <td>-5.485</td>\n",
       "      <td>90.654</td>\n",
       "      <td>4</td>\n",
       "      <td>28.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.000</td>\n",
       "      <td>4.500</td>\n",
       "      <td>0.189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qp02yc</td>\n",
       "      <td>1</td>\n",
       "      <td>6:45</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-5.485</td>\n",
       "      <td>90.654</td>\n",
       "      <td>6</td>\n",
       "      <td>30.750</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1.105</td>\n",
       "      <td>6.750</td>\n",
       "      <td>0.284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  geohash6  day timestamp  demand    lat    lon  hour  timestamp_hour  \\\n",
       "0   qp02yc    1      2:45   0.021 -5.485 90.654     2          26.750   \n",
       "1   qp02yc    1       3:0   0.010 -5.485 90.654     3          27.000   \n",
       "2   qp02yc    1       4:0   0.007 -5.485 90.654     4          28.000   \n",
       "3   qp02yc    1      4:30   0.004 -5.485 90.654     4          28.500   \n",
       "4   qp02yc    1      6:45   0.011 -5.485 90.654     6          30.750   \n",
       "\n",
       "   lat_scaled  lon_scaled   d_t  sample_weight  timestamp_decimal  \\\n",
       "0       0.000       0.171 0.021          1.221              2.750   \n",
       "1       0.000       0.171 0.010          1.105              3.000   \n",
       "2       0.000       0.171 0.007          1.105              4.000   \n",
       "3       0.000       0.171 0.004          1.000              4.500   \n",
       "4       0.000       0.171 0.011          1.105              6.750   \n",
       "\n",
       "   timestamp_decimal_scaled  \n",
       "0                     0.116  \n",
       "1                     0.126  \n",
       "2                     0.168  \n",
       "3                     0.189  \n",
       "4                     0.284  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "training_set = pd.read_csv('../dataset/training.csv')\n",
    "training_set['lat_lon'] = training_set['geohash6'].apply(lambda x: geohash.decode(x))\n",
    "training_set['lat'] = training_set['lat_lon'].apply(lambda x: x[0])\n",
    "training_set['lon'] = training_set['lat_lon'].apply(lambda x: x[1])\n",
    "training_set['hour'] = training_set['timestamp'].apply(lambda x: x.split(':')[0]).astype('int')\n",
    "training_set['timestamp_hour'] = training_set.apply(get_timestamp_hour, axis=1)\n",
    "training_set['lat_scaled'] = scaler.fit_transform(training_set['lat'].values.reshape(-1, 1))\n",
    "training_set['lon_scaled'] = scaler.fit_transform(training_set['lon'].values.reshape(-1, 1))\n",
    "\n",
    "training_set['d_t'] = training_set['demand']\n",
    "training_set['sample_weight'] = training_set['demand'].apply(get_exp_sample_weight)\n",
    "training_set['timestamp_decimal'] = training_set['timestamp'].apply(get_timestamp_decimal)\n",
    "training_set['timestamp_decimal_scaled'] = scaler.fit_transform(training_set['timestamp_decimal'].values.reshape(-1, 1))\n",
    "training_set = training_set.sort_values(by=['geohash6', 'timestamp_hour']).reset_index(drop=True)\n",
    "training_set = training_set.drop(columns='lat_lon')\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1329 processed\n",
      "51/1329 processed\n",
      "101/1329 processed\n",
      "151/1329 processed\n",
      "201/1329 processed\n",
      "251/1329 processed\n",
      "301/1329 processed\n",
      "351/1329 processed\n",
      "401/1329 processed\n",
      "451/1329 processed\n",
      "501/1329 processed\n",
      "551/1329 processed\n",
      "601/1329 processed\n",
      "651/1329 processed\n",
      "701/1329 processed\n",
      "751/1329 processed\n",
      "801/1329 processed\n",
      "851/1329 processed\n",
      "901/1329 processed\n",
      "951/1329 processed\n",
      "1001/1329 processed\n",
      "1051/1329 processed\n",
      "1101/1329 processed\n",
      "1151/1329 processed\n",
      "1201/1329 processed\n",
      "1251/1329 processed\n",
      "1301/1329 processed\n"
     ]
    }
   ],
   "source": [
    "train_df = get_time_lags(training_set).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lag 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'ts_d_plus_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/traffic-management-aiforsea/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ts_d_plus_1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-083ad6d8a395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lag'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts_d_plus_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts_d_plus_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-------------------------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/traffic-management-aiforsea/venv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/traffic-management-aiforsea/venv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ts_d_plus_1'"
     ]
    }
   ],
   "source": [
    "for lag in range(1, 6):\n",
    "    print('lag', lag)\n",
    "    print(train_df['ts_d_plus_{}'.format(lag)].drop_duplicates().min())\n",
    "    print(train_df['ts_d_plus_{}'.format(lag)].drop_duplicates().max())\n",
    "    print('-------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104      0.000\n",
       "132      0.250\n",
       "686      0.500\n",
       "422      0.750\n",
       "33       1.000\n",
       "34       1.250\n",
       "35       1.500\n",
       "80       1.750\n",
       "81       2.000\n",
       "52       2.250\n",
       "82       2.500\n",
       "276      2.750\n",
       "128      3.000\n",
       "129      3.250\n",
       "14       3.500\n",
       "46       3.750\n",
       "15       4.000\n",
       "7        4.250\n",
       "2        4.500\n",
       "37       4.750\n",
       "3        5.000\n",
       "8        5.250\n",
       "38       5.500\n",
       "39       5.750\n",
       "20       6.000\n",
       "58       6.250\n",
       "9        6.500\n",
       "10       6.750\n",
       "28       7.000\n",
       "48       7.250\n",
       "184      7.500\n",
       "49       7.750\n",
       "11       8.000\n",
       "29       8.250\n",
       "12       8.500\n",
       "4        8.750\n",
       "60       9.000\n",
       "61       9.250\n",
       "62       9.500\n",
       "13       9.750\n",
       "64      10.000\n",
       "22      10.250\n",
       "100     10.500\n",
       "23      10.750\n",
       "24      11.000\n",
       "0       11.250\n",
       "25      11.500\n",
       "31      11.750\n",
       "1       12.000\n",
       "173     12.250\n",
       "75      12.500\n",
       "320     12.750\n",
       "26      13.000\n",
       "103     13.250\n",
       "323     13.500\n",
       "673     13.750\n",
       "530     14.000\n",
       "1416    14.250\n",
       "863     14.500\n",
       "1049    14.750\n",
       "1032    15.000\n",
       "951     15.250\n",
       "1051    15.500\n",
       "1191    15.750\n",
       "2796    16.000\n",
       "2667    16.250\n",
       "2866    16.500\n",
       "2847    16.750\n",
       "2828    17.000\n",
       "2797    17.250\n",
       "2798    17.500\n",
       "2990    17.750\n",
       "3035    18.000\n",
       "2850    18.250\n",
       "3085    18.500\n",
       "23644   18.750\n",
       "2991    19.000\n",
       "39255   19.250\n",
       "39256   19.500\n",
       "49526   19.750\n",
       "3189    20.000\n",
       "32558   20.250\n",
       "2830    20.500\n",
       "49747   20.750\n",
       "31946   21.000\n",
       "52307   21.250\n",
       "51164   21.500\n",
       "12376   21.750\n",
       "5304    22.000\n",
       "76      22.250\n",
       "461     22.500\n",
       "6       22.750\n",
       "32      23.000\n",
       "68      23.250\n",
       "127     23.500\n",
       "69      23.750\n",
       "Name: timestamp_decimal, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['timestamp_decimal'].drop_duplicates().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet('../dataset/training_transformed.snappy.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in range(1, 11):\n",
    "    train_df['ts_d_minus_{}'.format(lag)] = train_df['timestamp_decimal'] \\\n",
    "        .apply(lambda x: create_ts_decimal_lag(x, lag))\n",
    "    train_df['ts_d_minus_{}_scaled'.format(lag)] = scaler \\\n",
    "        .fit_transform(train_df['ts_d_minus_{}'.format(lag)].values.reshape(-1, 1))\n",
    "train_df = train_df[sorted(train_df.columns)]\n",
    "train_df.to_parquet('../dataset/training_transformed.snappy.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['ts_d_minus_2', 'ts_d_minus_2_scaled']\n",
    "train_df[cols].drop_duplicates().sort_values(by=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[train_df.sample_weight > 2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set[['lat', 'lon']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['demand'].describe([0.75, 0.8, 0.85, 0.9, 0.95, 0.96, 0.97, 0.98, 0.985, 0.986, 0.99, 0.999, 0.9999, 0.99999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set \\\n",
    "    .groupby('geohash6') \\\n",
    "    .agg({'timestamp_hour': 'count'}) \\\n",
    "    .reset_index() \\\n",
    "    .sort_values(by='timestamp_hour', ascending=False) \\\n",
    "    .head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set['timestamp_decimal'] = training_set['timestamp'].apply(get_timestamp_decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.loc[(training_set.geohash6 == 'qp03wz') &\n",
    "                 (training_set.day == 7)\n",
    "                ].plot(kind='scatter', x='timestamp_decimal', y='demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heatmap of latlon to see where most of the activity is located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.loc[training_set.demand >= 0.5] \\\n",
    "    .groupby(['hour']) \\\n",
    "    .agg({'geohash6': 'count'}) \\\n",
    "    .reset_index() \\\n",
    "    .rename(columns={'geohash6': 'count'}) \\\n",
    "    .sort_values(by='count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = gmaps.figure()\n",
    "#locations = shuffle(training_set[['lat', 'lon', 'demand']]).iloc[:500000]\n",
    "locations = training_set.loc[\n",
    "    (training_set.hour == 10) & (training_set.day == 1)].copy()\n",
    "locations['demand_scaled'] = locations['demand'] * 100\n",
    "heatmap_layer = gmaps.symbol_layer(\n",
    "    locations[['lat', 'lon']], \n",
    "    fill_color='red')\n",
    "    #scale=locations['demand'])\n",
    "fig.add_layer(heatmap_layer)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing completeness of days and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_hour = training_set[['day', 'timestamp']]\n",
    "day_hour.groupby(by=['day', 'timestamp']).size().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hourly Demand Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 3, figsize=(80,100))\n",
    "fig.suptitle('Hourly distributions', fontsize=50)\n",
    "hour_index = 0\n",
    "for row_idx in range(0, 8):\n",
    "    for i in range(hour_index, hour_index+3):\n",
    "        to_plot = training_set[training_set.hour == i]['demand'] \n",
    "        axs[row_idx, i % 3].hist(to_plot, bins=100)\n",
    "        axs[row_idx, i % 3].set_title('Demand distribution for hour {}'.format(i), fontsize=40)\n",
    "        axs[row_idx, i % 3].tick_params(labelsize=30)\n",
    "    hour_index +=3\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set.groupby(['day', 'timestamp']).size().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic-management",
   "language": "python",
   "name": "traffic-management"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
